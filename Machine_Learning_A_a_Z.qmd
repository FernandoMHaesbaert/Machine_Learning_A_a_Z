---
title: "Machine_Learning_A_a_Z"
author:
      name: Dr. Fernando Machado Haesbaert
      affiliation: : Pesquisador em Ci√™ncia de Dados - EMBRAPA Pantanal
title-block-style: default
title-block-banner: false
format:
  html:
    toc: true            # ativa o sum√°rio
    toc-depth: 3         # n√≠vel de profundidade do sum√°rio
    toc-location: left   # sum√°rio lateral (pode ser 'right' ou 'floating')
    number-sections: true  # numera√ß√£o autom√°tica das se√ß√µes
    theme: cosmo         # tema do Bootstrap (cosmo √© s√≥ uma sugest√£o elegante)
    df-print: paged      # melhora a impress√£o de dataframes no html
    code-fold: true      # permite recolher/expandir blocos de c√≥digo
    code-tools: true     # adiciona ferramentas de c√≥pia/executar
    highlight-style: github  # estilo de realce de c√≥digo
    smooth-scroll: true      # rolagem suave ao clicar no sum√°rio
editor: visual
bibliography: referencias.bib
---

**APRENDIZADO DE M√ÅQUINA DE A-Z**  
Este gloss√°rio abrange conceitos, algoritmos e t√©cnicas fundamentais em aprendizado de m√°quina, organizados de A a Z. Cada entrada inclui uma breve descri√ß√£o, exemplos em R e refer√™ncias para aprofundamento.  

```{r}
#| message: false
#| warning: false
#| include: false
# Pacotes utilizados nos chunks
pkgs <- c("nnet", "randomForest", "caret", "rpart", "rpart.plot", "SuperLearner", 
          "FNN", "glmnet", "e1071", "xgboost", "recipes", "isotree", "keras", 
          "mlr3tuning", "ReinforcementLearning")
suppressPackageStartupMessages(lapply(pkgs, \(p) if (!requireNamespace(p, quietly = TRUE)) install.packages(p)))
```

# A 
## Artificial Neural Networks (ANNs)
S√£o sistemas de computa√ß√£o inspirados na arquitetura de neur√¥nios biol√≥gicos, formalizados como composi√ß√µes de fun√ß√µes n√£o-lineares $Œ∏(ùêñùê± + ùêõ)$, onde $ùêñ$ denota matrizes de pesos sin√°pticos, $Œ∏$ √© uma fun√ß√£o de ativa√ß√£o (ReLU, sigm√≥ide, tanh) e $b$  vi√©s (bias) . A capacidade de aproximadores universais (HORNIK et al., 1989) garante que ANNs com uma camada oculta suficientemente larga podem modelar qualquer fun√ß√£o cont√≠nua em dom√≠nio compacto.  

Exemplo em R ‚Äì rede feed-forward para regress√£o com pacote nnet:
```{r}
library(tidyverse)
library(nnet)
set.seed(123)
# 1. Gera√ß√£o de Dados N√ÉO-LINEARES: Redes neurais brilham onde a linearidade falha. 
# Vamos criar uma fun√ß√£o seno com ru√≠do.
N <- 500
x <- runif(N, 0, 10) # Valores entre 0 e 10
y <- sin(x) + rnorm(N, sd = 0.2) # Seno + ru√≠do
dados <- data.frame(x = x, y = y)

# 2. Separa√ß√£o Treino/Teste (Boas pr√°ticas)
indices <- sample(1:N, size = 0.8 * N)
treino <- dados[indices, ]
teste  <- dados[-indices, ]

# 3. Ajuste do Modelo (Regress√£o)
# size: neur√¥nios na camada oculta
# linout = TRUE: essencial para regress√£o (sa√≠da linear, n√£o sigmoide)
modelo_ann <- nnet(y ~ x, 
                   data = treino, 
                   size = 10, 
                   linout = TRUE, 
                   decay = 0.001, 
                   maxit = 1000, 
                   trace = FALSE) # trace=FALSE limpa o console

# 4. Avalia√ß√£o
previsoes <- predict(modelo_ann, teste)

# C√°lculo do Erro Quadr√°tico M√©dio (MSE)
mse <- mean((teste$y - previsoes)^2)

cat("Erro Quadr√°tico M√©dio (MSE) no teste:", round(mse, 4), "\n")

# Visualiza√ß√£o 
# 1. Prepara√ß√£o: Adicionar as previs√µes ao data frame de teste
dados_plot <- teste %>%
  mutate(predito = previsoes)

# 2. Visualiza√ß√£o
ggplot(dados_plot, aes(x = x)) +
  # Camada dos dados REAIS
  # Note que definimos color="Real" DENTRO do aes() para criar a legenda
  geom_point(aes(y = y, color = "Real", shape = "Real"), 
             size = 2, alpha = 0.7) +
  # Camada dos dados PREVISTOS
  geom_point(aes(y = predito, color = "Previsto", shape = "Previsto"), 
             size = 3, stroke = 1.2) +
  # Personaliza√ß√£o manual para imitar o estilo do R Base (cinza/vermelho e c√≠rculo/x)
  scale_color_manual(name = "Legenda", 
                     values = c("Real" = "gray", "Previsto" = "red")) +
  scale_shape_manual(name = "Legenda", 
                     values = c("Real" = 19, "Previsto" = 4)) +
  # T√≠tulos e Tema
  labs(title = "ANN: Ajuste a Fun√ß√£o Seno",
       subtitle = "Compara√ß√£o entre dados reais e aproxima√ß√£o da rede neural",
       y = "Valor de Y",
       x = "Valor de X") +
  theme_minimal()
```

Refer√™ncia: [@hornik89]

# B 
## Bagging (Bootstrap Aggregating)
Proposto por Breiman (1996), reduz vari√¢ncia ao treinar modelos em m√∫ltiplas amostras bootstrap e agregar predi√ß√µes por vota√ß√£o majorit√°ria (classifica√ß√£o) ou m√©dia (regress√£o). A de-correla√ß√£o entre √°rvores √© cr√≠tica para ganhos de precis√£o.
Exemplo em R ‚Äì floresta aleat√≥ria com randomForest:
r
Copy
library(randomForest)
rf <- randomForest(Species ~ ., data = iris, ntree = 1000, importance = TRUE)
print(rf$err.rate[1000, "OOB"])
Refer√™ncia: Breiman, L. (1996). Bagging predictors. Machine Learning, 24(2), 123-140.


# C
## Cross-Validation (CV)
Estimador de erro de generaliza√ß√£o com vi√©s‚Äìvari√¢ncia controlado. O k-fold CV particiona os dados em k subconjuntos mutuamente exclusivos; cada parti√ß√£o √© usada uma vez como teste. O nested CV √© obrigat√≥rio quando h√° sintonia de hiper-par√¢metros para evitar otimismo de desempenho (Cawley & Talbot, 2010).
Exemplo em R ‚Äì valida√ß√£o cruzada aninhada com caret:
r
Copy
library(caret)
tc <- trainControl(method = "repeatedcv", number = 10, repeats = 5, search = "grid")
fit <- train(Class ~ ., data = Sonar, method = "svmRadial", tuneLength = 15, trControl = tc)
Refer√™ncia: Cawley, G. C. & Talbot, N. L. C. (2010). On over-fitting in model selection and subsequent selection bias in performance evaluation. JMLR, 11, 2079-2107.

# D
## Decision Trees (√Årvores de Decis√£o)
Modelos simb√≥licos que particionam recursivamente o espa√ßo de preditores via medidas de impureza (Gini, entropia ou erro de classifica√ß√£o). A complexidade √© controlada por poda cost-complexity (cp) ou restri√ß√µes de profundidade.
Exemplo em R ‚Äì √°rvore CART com rpart:
r
Copy
library(rpart)
arvore <- rpart(Kyphosis ~ Age + Number + Start, data = kyphosis, method = "class", cp = 0.01)
rpart.plot::rpart.plot(arvore)

# E
## Ensemble Learning
Princ√≠pio de combinar preditores fracos para formar um preditor forte. Al√©m de bagging e boosting, stacking (Wolpert, 1992) usa um meta-modelo para ponderar especialistas de base. Diverg√™ncia entre componentes √© essencial (Kuncheva & Whitaker, 2003).
Exemplo em R ‚Äì stacking com SuperLearner:
r
Copy
library(SuperLearner)
SL.library <- c("SL.glm", "SL.ranger", "SL.xgboost")
fitSL <- SuperLearner(Y = y, X = X, SL.library = SL.library, family = binomial())

# F
## Feature Scaling
Muitos algoritmos (SVM, redes neurais, k-NN) s√£o sens√≠veis √† magnitude das vari√°veis. A padroniza√ß√£o z-score (Œº = 0, œÉ = 1) ou normaliza√ß√£o min-max [0, 1] garante converg√™ncia num√©rica e interpretabilidade de penalidades regulares.
Exemplo em R ‚Äì pipeline com recipes:
r
Copy
library(recipes)
receita <- recipe(y ~ ., data = df) %>%
  step_normalize(all_numeric_predictors()) %>%
  prep()
df_scaled <- bake(receita, new_data = NULL)

# G
## Gradient Descent (GD)
Algoritmo de otimiza√ß√£o de primeira ordem que atualiza par√¢metros na dire√ß√£o oposta ao gradiente da fun√ß√£o de perda. GD estoc√°stico (SGD) reduz custo computacional por amostras mini-batch; adaptativos (Adam, RMSprop) ajustam taxas de aprendizado por momentos.
Exemplo em R ‚Äì GD manual para regress√£o linear:
r
Copy
gd <- function(X, y, lr = 0.01, epochs = 1000) {
  beta <- rep(0, ncol(X))
  for(i in 1:epochs) {
    grad <- -2 * t(X) %*% (y - X %*% beta) / nrow(X)
    beta <- beta - lr * grad
  }
  beta
}

# H
## Hyperparameter Tuning
Espa√ßo de configura√ß√µes externas ao modelo (n√∫mero de vizinhos, C de SVM, profundidade de √°rvore) √© otimizado via busca em grade, aleat√≥ria (Bergstra & Bengio, 2012) ou bayesiana (TPE, Gaussian Process).
Exemplo em R ‚Äì bayesiano com mlr3tuning:
r
Copy
library(mlr3tuning)
task <- tsk("sonar")
learner <- lrn("classif.svm", type = "C-classification", kernel = "radial")
search_space <- ps(C = p_dbl(1e-1, 1e3), sigma = p_dbl(1e-4, 1))
tuner <- tnr("mbo", n_evals = 30)

# I
## Instance-Based Learning
Fam√≠lia de algoritmos que adiam generaliza√ß√£o at√© o momento da predi√ß√£o, armazenando exemplos (k-NN, kernel regression). A complexidade de predi√ß√£o √© O(n), exigindo estruturas de indexa√ß√£o (KD-tree, ball-tree) ou proje√ß√µes de baixa dimensionalidade.
Exemplo em R ‚Äì k-NN com FNN:
r
Copy
library(FNN)
knn_pred <- knn.reg(train = treino[, -1], test = teste[, -1], y = treino$y, k = 5)

# J
## Jaccard Index
Medida de similaridade de conjuntos J(A,B) = |A ‚à© B| / |A ‚à™ B|. √ötil em dados bin√°rios (presen√ßa/aus√™ncia) ou comunidades em grafos.
Exemplo em R:
r
Copy
jaccard <- function(a, b) {
  length(intersect(a, b)) / length(union(a, b))
}

# K
## K-Nearest Neighbors (KNN)
Classificador n√£o-param√©trico que atribui classe majorit√°ria entre os k vizinhos mais pr√≥ximos. A escolha de k via CV balancea vi√©s‚Äìvari√¢ncia; dist√¢ncias ponderadas (IDW) reduzem influ√™ncia de vizinhos distantes.
Exemplo em R ‚Äì tuning de k:
r
Copy
library(caret)
fit <- train(Class ~ ., data = Sonar, method = "knn", tuneGrid = data.frame(k = seq(1, 25, by = 2)))

# L
## Logistic Regression
Modelo linear generalizado com fun√ß√£o de liga√ß√£o logit, garantindo probabilidades em [0, 1]. A interpreta√ß√£o via odds-ratio √© cl√°ssica em epidemiologia. Regulariza√ß√£o L1 (lasso) permite sele√ß√£o de vari√°veis.
Exemplo em R ‚Äì regress√£o log√≠stica com glmnet:
r
Copy
library(glmnet)
cvfit <- cv.glmnet(x = x, y = y, family = "binomial", alpha = 1)
coef(cvfit, s = "lambda.min")

# M
## Model Overfitting
Situa√ß√£o emindo a fun√ß√£o de risco emp√≠rico ‚â™ risco verdadeiro, indicando captura de ru√≠do. Diagn√≥stico via curva de valida√ß√£o (training √ó validation loss) ou complexidade VC. Solu√ß√µes: penaliza√ß√£o, early stopping, aumento de dados.
Refer√™ncia: Vapnik, V. (1998). Statistical Learning Theory. Wiley.

# N
## Normalization
Transforma√ß√£o que reescala vari√°veis para intervalo fixo, comum em redes neurais. A normaliza√ß√£o por lote (batch norm) acelera converg√™ncia reduzindo covari√¢ncia interna (Ioffe & Szegedy, 2015).
Exemplo em R ‚Äì batch norm em keras:
r
Copy
library(keras)
model %>% layer_batch_normalization()

# O
## Outliers
Observa√ß√µes que violam pressuposto de distribui√ß√£o cont√≠nua ou homocedasticidade. Detectados via dist√¢ncia de Mahalanobis, LOF ou Isolation Forest. Em modelos lineares, leverage e Cook‚Äôs D quantificam influ√™ncia.
Exemplo em R:
r
Copy
library(isotree)
iso <- isolation.forest(df, ntrees = 500)
scores <- predict(iso, df)
P ‚Äì Principal Component Analysis (PCA)
Decomposi√ß√£o ortogonal que maximiza vari√¢ncia projetada, equivalente √† decomposi√ß√£o em valores singulares (SVD) da matriz centrada. PCA √© usada para visualiza√ß√£o, ru√≠do filtrado e redu√ß√£o de dimensionalidade antes de algoritmos pesados.
Exemplo em R:
r
Copy
pca <- prcomp(df, center = TRUE, scale. = TRUE)
biplot(pca)
Q ‚Äì Q-Learning
Algoritmo de aprendizado por refor√ßo off-policy que estima fun√ß√£o Q*(s,a) via atualiza√ß√£o de Bellman. A converg√™ncia √© garantida sob visita√ß√£o infinita de pares (s,a) com taxa de aprendizado decrescente.
Exemplo em R ‚Äì pacote ReinforcementLearning:
r
Copy
library(ReinforcementLearning)
data("gridworld")
model <- ReinforcementLearning(data = gridworld, s = "state", a = "action", r = "reward", s_new = "next_state")
R ‚Äì Regularization
T√©cnica que adiciona penalidade Œ©(Œ≤) √† fun√ß√£o de perda, impondo trade-off vi√©s‚Äìvari√¢ncia. Ridge (L2) encolhe coeficientes, lasso (L1) promove esparsidade, elastic-net combina ambas.
Exemplo em R ‚Äì elastic-net com glmnet:
r
Copy
cvfit <- cv.glmnet(x, y, alpha = 0.5)  # 0.5 = elastic-net
S ‚Äì Support Vector Machines (SVM)
Classificador de margem m√°xima que resolve problema de otimiza√ß√£o quadr√°tica convexo. O kernel trick permite espa√ßos de Hilbert de alta dimens√£o sem custo computacional expl√≠cito.
Exemplo em R ‚Äì SVM radial com e1071:
r
Copy
library(e1071)
fit <- svm(Class ~ ., data = treino, kernel = "radial", cost = 10, gamma = 0.1)
T ‚Äì Training Set
Conjunto de dados usado para estimar par√¢metros do modelo. Deve ser representativo da popula√ß√£o; t√©cnicas de amostragem estratificada preservam propor√ß√µes de classe. O tamanho ideal depende da taxa de cobertura do espa√ßo de caracter√≠sticas (Cover & Hart, 1967).
U ‚Äì Underfitting
Modelo com capacidade insuficiente (alto vi√©s) n√£o captura estrutura dos dados. Indicado por erro de treino elevado. Solu√ß√µes: aumentar complexidade, adicionar features, reduzir regulariza√ß√£o.
V ‚Äì Validation Set
Dados separados do treino para ajuste de hiper-par√¢metros, evitando vazamento de informa√ß√£o. Em datasets pequenos, cross-validation interno √© prefer√≠vel.
W ‚Äì Weight Initialization
Valores iniciais influenciam converg√™ncia e qualidade de m√≠nimo. Xavier/Glorot idealiza vari√¢ncia 2/(fan_in + fan_out) para sigm√≥ides; He inicializa com 2/fan_in para ReLU.
Exemplo em R ‚Äì inicializa√ß√£o He no keras:
r
Copy
layer_dense(units = 128, activation = "relu", kernel_initializer = "he_normal")
X ‚Äì XGBoost
Implementa√ß√£o escalon√°vel de gradient boosting comÁ®ÄÁñèÊï∞ÊçÆ (sparsity-aware), regulariza√ß√£o L1/L2 em √°rvores, e paraleliza√ß√£o porÁâπÂæÅÂàÜÂùó. Domina competi√ß√µes tabulares.
Exemplo em R:
r
Copy
library(xgboost)
dtrain <- xgb.DMatrix(data = as.matrix(treino[, -1]), label = treino$label)
param <- list(max_depth = 6, eta = 0.05, subsample = 0.8, colsample_bytree = 0.8, objective = "binary:logistic")
bst <- xgb.train(param, dtrain, nrounds = 400, watchlist = list(train = dtrain), early_stopping_rounds = 10)
Y ‚Äì Y-Axis
Nos gr√°ficos de curvas de aprendizado, representa m√©trica de desempenho (acur√°cia, AUC, RMSE) ou erro emp√≠rico. A an√°lise de gap entre treino e valida√ß√£o revela overfitting/underfitting.
Z ‚Äì Z-Score
Escore padronizado z = (x ‚àí Œº)/œÉ, indicando desvios em unidades de desvio-padr√£o. Utilizado para detec√ß√£o de outliers (|z| > 3) e normaliza√ß√£o de features.
Exemplo em R:
r
Copy
z <- scale(df$x)
Refer√™ncias gerais
Bergstra, J. & Bengio, Y. (2012). Random search for hyper-parameter optimization. JMLR, 13, 281-305.
Breiman, L. (2001). Random forests. Machine Learning, 45(1), 5-32.
Cover, T. & Hart, P. (1967). Nearest neighbor pattern classification. IEEE TIT, 13(1), 21-27.
Ioffe, S. & Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. ICML.