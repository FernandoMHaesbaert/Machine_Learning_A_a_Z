---
title: "Machine Learning de A a Z"
author:
  - name: "Dr. Fernando Machado Haesbaert"
    affiliation: "Pesquisador em Ci√™ncia de Dados"
title-block-style: default
title-block-banner: false

format:
  html:
    toc: true
    toc-depth: 3
    toc-location: left
    number-sections: true
    theme: cosmo
    df-print: paged
    code-fold: true
    code-tools: true
    highlight-style: github
    smooth-scroll: true

editor: visual
bibliography: referencias.bib
---

Aprendizado de M√°quina (`Machine Learning`) √© a ci√™ncia que estuda algoritmos e modelos estat√≠sticos que permitem aos computadores realizar tarefas espec√≠ficas sem serem explicitamente programados para isso conceito definido por Arthur Samuel em 1959 [@ArthurSamuel59]. Esses sistemas aprendem a partir de dados, identificando padr√µes e fazendo previs√µes com base nesses dados.\
O aprendizado de m√°quina √© amplamente utilizado em diversas √°reas, como reconhecimento de voz, vis√£o computacional, recomenda√ß√£o de produtos, an√°lise preditiva e muito mais. Existem diferentes tipos de aprendizado de m√°quina, incluindo aprendizado supervisionado, n√£o supervisionado e por refor√ßo, cada um com suas pr√≥prias t√©cnicas e aplica√ß√µes.\
Este gloss√°rio abrange conceitos, termos, algoritmos e t√©cnicas fundamentais em aprendizado de m√°quina, organizados de A a Z. Cada entrada inclui uma breve descri√ß√£o, exemplos em R e refer√™ncias para aprofundamento.

```{r}
#| message: false
#| warning: false
#| include: false
# Pacotes utilizados nos chunks
pkgs <- c("nnet", "randomForest", "caret", "rpart", "rpart.plot", "SuperLearner", 
          "FNN", "glmnet", "e1071", "xgboost", "recipes", "isotree", "keras", 
          "mlr3tuning", "ReinforcementLearning", "scales")
suppressPackageStartupMessages(lapply(pkgs, \(p) if (!requireNamespace(p, quietly = TRUE)) install.packages(p)))
lapply(pkgs, library, character.only = TRUE)

```

# A

## Accuracy (Acur√°cia)

M√©trica de avalia√ß√£o para problemas de classifica√ß√£o, definida como a propor√ß√£o de previs√µes corretas. Embora simples, costuma ser enganosa em bases desbalanceadas. A acur√°cia √© calculada como: $$\text{Acur√°cia} = \frac{TP + TN}{TP + TN + FP + FN}$$ Onde:\
$TP$ s√£o verdadeiros positivos\
$TN$ verdadeiros negativos\
$FP$ falsos positivos\
$FN$ falsos negativos

### Exemplo: C√°lculo da Acur√°cia em R

```{r}
matriz_confusao <- matrix(c(50, 2, 1, 47), nrow = 2, byrow = TRUE)
colnames(matriz_confusao) <- c("Previsto Positivo", "Previsto Negativo")
rownames(matriz_confusao) <- c("Real Positivo", "Real Negativo")
# Visualizar a matriz de confus√£o 
matriz_confusao |> as.data.frame() |> 
  knitr::kable(col.names = c("Real/Previsto", "Previsto Positivo", "Previsto Negativo"),
               caption = "Matriz de Confus√£o") |> 
  kableExtra::kable_styling(full_width = FALSE)
# C√°lculo da Acur√°cia
TP <- matriz_confusao[1, 1]
TN <- matriz_confusao[2, 2]
acuracia <- (TP + TN) / sum(matriz_confusao)
cat("Acur√°cia:", percent(acuracia, accuracy = 0.01), "\n")
```

Veja mais detalhes em: [@kuhn13]

## AdaBoost (Adaptive Boosting)

Algoritmo de ensemble baseado em boosting sequencial. Modelos fracos s√£o combinados com pesos adaptativos para reduzir o erro. Cada modelo subsequente foca nos erros do anterior, ajustando pesos das amostras, modelo proposto por [@freund97].

### Exemplo: algoritmo AdaBoost, para classificar flores do conjunto de dados Iris.

```{r}
set.seed(123)
# 1. Gera√ß√£o de Dados: Usaremos o conjunto de dados iris
data(iris)
# 2. Dividir em treino e teste
id <- sample(1:nrow(iris), size = 0.7 * nrow(iris))
treino <- iris[id, ]
teste  <- iris[-id, ]
# 3. Ajuste do Modelo AdaBoost
# install.packages("adabag")
library(adabag)
modelo <- boosting(Species ~ ., data = treino)
# 4. Avalia√ß√£o
previsoes <- predict.boosting(modelo, newdata = teste[, -5])
# Corre√ß√£o: Garante que os n√≠veis sejam id√™nticos
previsoes_fator <- factor(previsoes$class, levels = levels(teste$Species))

# 5. Matriz de Confus√£o no conjunto de teste
confusao_df <- confusionMatrix(
  data = previsoes_fator,      # Usando a previs√£o corrigida
  reference = teste$Species   # Os valores reais
)
knitr::kable(
  confusao_df$table,
  caption = "Matriz de Confus√£o - AdaBoost"
) |> 
  kableExtra::kable_styling(full_width = FALSE)
# 6. C√°lculo da Acur√°cia
acuracia <- sum(previsoes_fator == teste$Species) / nrow(teste)
cat("Acur√°cia do AdaBoost no conjunto iris:", percent(acuracia, accuracy = 0.01), "\n")
```

Uma refer√™ncia detalhada √©: [@hastie09], este livro √© considerado a b√≠blia do aprendizado de m√°quina e dedica um cap√≠tulo inteiro (Chapter 10: Boosting and Additive Trees) ao conceito de Boosting e ao AdaBoost, explicando-o de forma rigorosa e detalhada, com foco em sua base estat√≠stica e matem√°tica.

## Aglomerativo Hierarchical Clustering
Fam√≠lia hier√°rquica de m√©todos de agrupamento que unem observa√ß√µes progressivamente. Come√ßa com cada ponto como um cluster individual e funde os mais pr√≥ximos iterativamente, formando uma √°rvore dendrograma de baixo para cima.  
Crit√©rios de liga√ß√£o (single, complete, average) definem a dist√¢ncia entre clusters.  

### Exemplo em R ‚Äì clustering aglomerativo com hclust: 
```{r}
dist_matrix <- dist(iris[, -5]) 
hc <- hclust(dist_matrix, method = "complete") 
plot(hc) # Dendrograma
```

Veja o metodo contr√°rio a esse , o [Divisive Hierarchical Clustering](#divisive_hierarchical_clustering).

Vejam mais detalhes em [@Han23], onde o cap√≠tulo aborda sobre `Cluster Analysis` (An√°lise de Agrupamento) aborda a Agrupamento Hier√°rquico (Se√ß√£o 8.3) e explica detalhadamente como o algoritmo aglomerativo (bottom-up) funciona, a constru√ß√£o do dendrograma e a diferen√ßa entre os crit√©rios de liga√ß√£o (single-link, complete-link e average-link).  

## AIC ‚Äì Akaike Information Criterion (Crit√©rio de Akaike)
√çndice de sele√ß√£o de modelos usado amplamente em modelos estat√≠sticos e regress√µes generalizadas. Baseia-se na teoria da informa√ß√£o, penalizando a complexidade do modelo para evitar overfitting.  
Calculado como: 
$$\text{AIC} = 2k - 2\ln(L)$$  
Onde: $k$ √© o n√∫mero de par√¢metros do modelo e $L$ √© a verossimilhan√ßa m√°xima do modelo.  
Modelos com menor AIC s√£o preferidos.  

### Exemplo: compara√ß√£o de modelos lineares com AIC: 
```{r}
# Repare que removi os sinais de "+" que indicam quebra de linha no console
df <- tibble( 
  x1 = rnorm(100, mean = 5, sd = 2),
  # Aqui o tibble permite chamar o x1 criado na linha acima
  x2 = x1 + rnorm(100, mean = 3, sd = 2), 
  x3 = x2 + rnorm(100, mean = 3, sd = 2)
) |> 
  mutate(
    y = 3 + 2*x1 - 1.5*x2 + 0.5*x3 + rnorm(100, mean=0, sd=2)
  )
cor(df)
model1 <- lm(y ~ x1 + x2, data = df) 
model2 <- lm(y ~ x1 + x2 + x3, data = df) 
aic_values <- AIC(model1, model2) 
print(aic_values)
```



## Artificial Neural Networks (ANNs)

S√£o sistemas de computa√ß√£o inspirados na arquitetura de neur√¥nios biol√≥gicos, formalizados como composi√ß√µes de fun√ß√µes n√£o-lineares $Œ∏(ùêñùê± + ùêõ)$, onde $ùêñ$ denota matrizes de pesos sin√°pticos, $Œ∏$ √© uma fun√ß√£o de ativa√ß√£o (ReLU, sigm√≥ide, tanh) e $b$ vi√©s (bias) . A capacidade de aproximadores universais (HORNIK et al., 1989) garante que ANNs com uma camada oculta suficientemente larga podem modelar qualquer fun√ß√£o cont√≠nua em dom√≠nio compacto.\
Refer√™ncia: [@hornik89].

### Exemplo em R: Rede feed-forward para regress√£o com pacote `{nnet}`

A rede feed-forward √© um tipo de rede neural artificial onde as conex√µes entre os n√≥s n√£o formam ciclos. Ela √© amplamente utilizada para tarefas de regress√£o e classifica√ß√£o. No R, o pacote `{nnet}` oferece uma maneira simples de implementar redes feed-forward.

```{r}
set.seed(123)
# 1. Gera√ß√£o de Dados N√ÉO-LINEARES: Redes neurais brilham onde a linearidade falha. 
# Vamos criar uma fun√ß√£o seno com ru√≠do.
N <- 500
x <- runif(N, 0, 10) # Valores entre 0 e 10
y <- sin(x) + rnorm(N, sd = 0.2) # Seno + ru√≠do
dados <- data.frame(x = x, y = y)

# 2. Separa√ß√£o Treino/Teste (Boas pr√°ticas)
indices <- sample(1:N, size = 0.8 * N)
treino <- dados[indices, ]
teste  <- dados[-indices, ]

# 3. Ajuste do Modelo (Regress√£o)
# size: neur√¥nios na camada oculta
# linout = TRUE: essencial para regress√£o (sa√≠da linear, n√£o sigmoide)
modelo_ann <- nnet(y ~ x, 
                   data = treino, 
                   size = 10, 
                   linout = TRUE, 
                   decay = 0.001, 
                   maxit = 1000, 
                   trace = FALSE) # trace=FALSE limpa o console

# 4. Avalia√ß√£o
previsoes <- predict(modelo_ann, teste)

# C√°lculo do Erro Quadr√°tico M√©dio (MSE)
mse <- mean((teste$y - previsoes)^2)

cat("Erro Quadr√°tico M√©dio (MSE) no teste:", round(mse, 4), "\n")

# Visualiza√ß√£o 
# 1. Prepara√ß√£o: Adicionar as previs√µes ao data frame de teste
dados_plot <- teste %>%
  mutate(predito = previsoes)

# 2. Visualiza√ß√£o
ggplot(dados_plot, aes(x = x)) +
  # Camada dos dados REAIS
  # Note que definimos color="Real" DENTRO do aes() para criar a legenda
  geom_point(aes(y = y, color = "Real", shape = "Real"), 
             size = 2, alpha = 0.7) +
  # Camada dos dados PREVISTOS
  geom_point(aes(y = predito, color = "Previsto", shape = "Previsto"), 
             size = 3, stroke = 1.2) +
  # Personaliza√ß√£o manual para imitar o estilo do R Base (cinza/vermelho e c√≠rculo/x)
  scale_color_manual(name = "Legenda", 
                     values = c("Real" = "gray", "Previsto" = "red")) +
  scale_shape_manual(name = "Legenda", 
                     values = c("Real" = 19, "Previsto" = 4)) +
  # T√≠tulos e Tema
  labs(title = "ANN: Ajuste a Fun√ß√£o Seno",
       subtitle = "Compara√ß√£o entre dados reais e aproxima√ß√£o da rede neural",
       y = "Valor de Y",
       x = "Valor de X") +
  theme_minimal()
```

# B

## Bagging (Bootstrap Aggregating)

Proposto por Breiman (1996), reduz vari√¢ncia ao treinar modelos em m√∫ltiplas amostras bootstrap e agregar predi√ß√µes por vota√ß√£o majorit√°ria (classifica√ß√£o) ou m√©dia (regress√£o). A de-correla√ß√£o entre √°rvores √© cr√≠tica para ganhos de precis√£o. Exemplo em R ‚Äì floresta aleat√≥ria com randomForest: r Copy library(randomForest) rf \<- randomForest(Species \~ ., data = iris, ntree = 1000, importance = TRUE) print(rf\$err.rate\[1000, "OOB"\]) Refer√™ncia: Breiman, L. (1996). Bagging predictors. Machine Learning, 24(2), 123-140.

# C

## Cross-Validation (CV)

Estimador de erro de generaliza√ß√£o com vi√©s‚Äìvari√¢ncia controlado. O k-fold CV particiona os dados em k subconjuntos mutuamente exclusivos; cada parti√ß√£o √© usada uma vez como teste. O nested CV √© obrigat√≥rio quando h√° sintonia de hiper-par√¢metros para evitar otimismo de desempenho (Cawley & Talbot, 2010). Exemplo em R ‚Äì valida√ß√£o cruzada aninhada com caret: r Copy library(caret) tc \<- trainControl(method = "repeatedcv", number = 10, repeats = 5, search = "grid") fit \<- train(Class \~ ., data = Sonar, method = "svmRadial", tuneLength = 15, trControl = tc) Refer√™ncia: Cawley, G. C. & Talbot, N. L. C. (2010). On over-fitting in model selection and subsequent selection bias in performance evaluation. JMLR, 11, 2079-2107.

# D

## Decision Trees (√Årvores de Decis√£o)

Modelos simb√≥licos que particionam recursivamente o espa√ßo de preditores via medidas de impureza (Gini, entropia ou erro de classifica√ß√£o). A complexidade √© controlada por poda cost-complexity (cp) ou restri√ß√µes de profundidade. Exemplo em R ‚Äì √°rvore CART com rpart: r Copy library(rpart) arvore \<- rpart(Kyphosis \~ Age + Number + Start, data = kyphosis, method = "class", cp = 0.01) rpart.plot::rpart.plot(arvore)

## Divisive Hierarchical Clustering{#divisive_hierarchical_clustering}
Um m√©todo de agrupamento hier√°rquico divisivo emprega uma estrat√©gia de cima para baixo. Ele come√ßa colocando todos os objetos em um cluster, que √© a raiz da hierarquia. Em seguida, divide o cluster raiz em v√°rios subclusters menores e particiona recursivamente esses clusters em clusters menores. O processo de particionamento continua at√© que cada objeto esteja em seu pr√≥prio cluster ou at√© que um crit√©rio de parada seja atendido (por exemplo, n√∫mero desejado de clusters ou dist√¢ncia m√≠nima entre clusters).  

### Exemplo em R ‚Äì clustering divisivo com diana
```{r}
library(cluster) 
data(iris) 
div_clust <- diana(iris[, -5]) 
plot(div_clust) # Dendrograma
```



# E

## Ensemble Learning

Princ√≠pio de combinar preditores fracos para formar um preditor forte. Al√©m de bagging e boosting, stacking (Wolpert, 1992) usa um meta-modelo para ponderar especialistas de base. Diverg√™ncia entre componentes √© essencial (Kuncheva & Whitaker, 2003). Exemplo em R ‚Äì stacking com SuperLearner: r Copy library(SuperLearner) SL.library \<- c("SL.glm", "SL.ranger", "SL.xgboost") fitSL \<- SuperLearner(Y = y, X = X, SL.library = SL.library, family = binomial())

# F

## Feature Scaling

Muitos algoritmos (SVM, redes neurais, k-NN) s√£o sens√≠veis √† magnitude das vari√°veis. A padroniza√ß√£o z-score (Œº = 0, œÉ = 1) ou normaliza√ß√£o min-max \[0, 1\] garante converg√™ncia num√©rica e interpretabilidade de penalidades regulares. Exemplo em R ‚Äì pipeline com recipes: r Copy library(recipes) receita \<- recipe(y \~ ., data = df) %\>% step_normalize(all_numeric_predictors()) %\>% prep() df_scaled \<- bake(receita, new_data = NULL)

# G

## Gradient Descent (GD)

Algoritmo de otimiza√ß√£o de primeira ordem que atualiza par√¢metros na dire√ß√£o oposta ao gradiente da fun√ß√£o de perda. GD estoc√°stico (SGD) reduz custo computacional por amostras mini-batch; adaptativos (Adam, RMSprop) ajustam taxas de aprendizado por momentos. Exemplo em R ‚Äì GD manual para regress√£o linear: r Copy gd \<- function(X, y, lr = 0.01, epochs = 1000) { beta \<- rep(0, ncol(X)) for(i in 1:epochs) { grad \<- -2 \* t(X) %*% (y - X %*% beta) / nrow(X) beta \<- beta - lr \* grad } beta }

# H

## Hyperparameter Tuning

Espa√ßo de configura√ß√µes externas ao modelo (n√∫mero de vizinhos, C de SVM, profundidade de √°rvore) √© otimizado via busca em grade, aleat√≥ria (Bergstra & Bengio, 2012) ou bayesiana (TPE, Gaussian Process). Exemplo em R ‚Äì bayesiano com mlr3tuning: r Copy library(mlr3tuning) task \<- tsk("sonar") learner \<- lrn("classif.svm", type = "C-classification", kernel = "radial") search_space \<- ps(C = p_dbl(1e-1, 1e3), sigma = p_dbl(1e-4, 1)) tuner \<- tnr("mbo", n_evals = 30)

# I

## Instance-Based Learning

Fam√≠lia de algoritmos que adiam generaliza√ß√£o at√© o momento da predi√ß√£o, armazenando exemplos (k-NN, kernel regression). A complexidade de predi√ß√£o √© O(n), exigindo estruturas de indexa√ß√£o (KD-tree, ball-tree) ou proje√ß√µes de baixa dimensionalidade. Exemplo em R ‚Äì k-NN com FNN: r Copy library(FNN) knn_pred \<- knn.reg(train = treino\[, -1\], test = teste\[, -1\], y = treino\$y, k = 5)

# J

## Jaccard Index

Medida de similaridade de conjuntos J(A,B) = \|A ‚à© B\| / \|A ‚à™ B\|. √ötil em dados bin√°rios (presen√ßa/aus√™ncia) ou comunidades em grafos. Exemplo em R: r Copy jaccard \<- function(a, b) { length(intersect(a, b)) / length(union(a, b)) }

# K

## K-Nearest Neighbors (KNN)

Classificador n√£o-param√©trico que atribui classe majorit√°ria entre os k vizinhos mais pr√≥ximos. A escolha de k via CV balancea vi√©s‚Äìvari√¢ncia; dist√¢ncias ponderadas (IDW) reduzem influ√™ncia de vizinhos distantes. Exemplo em R ‚Äì tuning de k: r Copy library(caret) fit \<- train(Class \~ ., data = Sonar, method = "knn", tuneGrid = data.frame(k = seq(1, 25, by = 2)))

# L

## Logistic Regression

Modelo linear generalizado com fun√ß√£o de liga√ß√£o logit, garantindo probabilidades em \[0, 1\]. A interpreta√ß√£o via odds-ratio √© cl√°ssica em epidemiologia. Regulariza√ß√£o L1 (lasso) permite sele√ß√£o de vari√°veis. Exemplo em R ‚Äì regress√£o log√≠stica com glmnet: r Copy library(glmnet) cvfit \<- cv.glmnet(x = x, y = y, family = "binomial", alpha = 1) coef(cvfit, s = "lambda.min")

# M

## Model Overfitting

Situa√ß√£o emindo a fun√ß√£o de risco emp√≠rico ‚â™ risco verdadeiro, indicando captura de ru√≠do. Diagn√≥stico via curva de valida√ß√£o (training √ó validation loss) ou complexidade VC. Solu√ß√µes: penaliza√ß√£o, early stopping, aumento de dados. Refer√™ncia: Vapnik, V. (1998). Statistical Learning Theory. Wiley.

# N

## Normalization

Transforma√ß√£o que reescala vari√°veis para intervalo fixo, comum em redes neurais. A normaliza√ß√£o por lote (batch norm) acelera converg√™ncia reduzindo covari√¢ncia interna (Ioffe & Szegedy, 2015). Exemplo em R ‚Äì batch norm em keras: r Copy library(keras) model %\>% layer_batch_normalization()

# O

## Outliers

Observa√ß√µes que violam pressuposto de distribui√ß√£o cont√≠nua ou homocedasticidade. Detectados via dist√¢ncia de Mahalanobis, LOF ou Isolation Forest. Em modelos lineares, leverage e Cook‚Äôs D quantificam influ√™ncia. Exemplo em R: r Copy library(isotree) iso \<- isolation.forest(df, ntrees = 500) scores \<- predict(iso, df)

# P

## Principal Component Analysis (PCA)

Decomposi√ß√£o ortogonal que maximiza vari√¢ncia projetada, equivalente √† decomposi√ß√£o em valores singulares (SVD) da matriz centrada. PCA √© usada para visualiza√ß√£o, ru√≠do filtrado e redu√ß√£o de dimensionalidade antes de algoritmos pesados. Exemplo em R: r Copy pca \<- prcomp(df, center = TRUE, scale. = TRUE) biplot(pca)

# Q

## Q-Learning

Algoritmo de aprendizado por refor√ßo off-policy que estima fun√ß√£o Q\*(s,a) via atualiza√ß√£o de Bellman. A converg√™ncia √© garantida sob visita√ß√£o infinita de pares (s,a) com taxa de aprendizado decrescente. Exemplo em R ‚Äì pacote ReinforcementLearning: r Copy library(ReinforcementLearning) data("gridworld") model \<- ReinforcementLearning(data = gridworld, s = "state", a = "action", r = "reward", s_new = "next_state")

# R

## Regularization

T√©cnica que adiciona penalidade Œ©(Œ≤) √† fun√ß√£o de perda, impondo trade-off vi√©s‚Äìvari√¢ncia. Ridge (L2) encolhe coeficientes, lasso (L1) promove esparsidade, elastic-net combina ambas. Exemplo em R ‚Äì elastic-net com glmnet: r Copy cvfit \<- cv.glmnet(x, y, alpha = 0.5) \# 0.5 = elastic-net

# S

## Support Vector Machines (SVM)

Classificador de margem m√°xima que resolve problema de otimiza√ß√£o quadr√°tica convexo. O kernel trick permite espa√ßos de Hilbert de alta dimens√£o sem custo computacional expl√≠cito. Exemplo em R ‚Äì SVM radial com e1071: r Copy library(e1071) fit \<- svm(Class \~ ., data = treino, kernel = "radial", cost = 10, gamma = 0.1)

# T

## Training Set

Conjunto de dados usado para estimar par√¢metros do modelo. Deve ser representativo da popula√ß√£o; t√©cnicas de amostragem estratificada preservam propor√ß√µes de classe. O tamanho ideal depende da taxa de cobertura do espa√ßo de caracter√≠sticas (Cover & Hart, 1967).

# U

## Underfitting

Modelo com capacidade insuficiente (alto vi√©s) n√£o captura estrutura dos dados. Indicado por erro de treino elevado. Solu√ß√µes: aumentar complexidade, adicionar features, reduzir regulariza√ß√£o.

# V

## Validation Set

Dados separados do treino para ajuste de hiper-par√¢metros, evitando vazamento de informa√ß√£o. Em datasets pequenos, cross-validation interno √© prefer√≠vel.

# W

## Weight Initialization

Valores iniciais influenciam converg√™ncia e qualidade de m√≠nimo. Xavier/Glorot idealiza vari√¢ncia 2/(fan_in + fan_out) para sigm√≥ides; He inicializa com 2/fan_in para ReLU. Exemplo em R ‚Äì inicializa√ß√£o He no keras: r Copy layer_dense(units = 128, activation = "relu", kernel_initializer = "he_normal")

# X

## XGBoost

Implementa√ß√£o escalon√°vel de gradient boosting comÁ®ÄÁñèÊï∞ÊçÆ (sparsity-aware), regulariza√ß√£o L1/L2 em √°rvores, e paraleliza√ß√£o porÁâπÂæÅÂàÜÂùó. Domina competi√ß√µes tabulares. Exemplo em R: r Copy library(xgboost) dtrain \<- xgb.DMatrix(data = as.matrix(treino\[, -1\]), label = treino\$label) param \<- list(max_depth = 6, eta = 0.05, subsample = 0.8, colsample_bytree = 0.8, objective = "binary:logistic") bst \<- xgb.train(param, dtrain, nrounds = 400, watchlist = list(train = dtrain), early_stopping_rounds = 10)

# Y

## Y-Axis

Nos gr√°ficos de curvas de aprendizado, representa m√©trica de desempenho (acur√°cia, AUC, RMSE) ou erro emp√≠rico. A an√°lise de gap entre treino e valida√ß√£o revela overfitting/underfitting.

# Z

## Z-Score

Escore padronizado z = (x ‚àí Œº)/œÉ, indicando desvios em unidades de desvio-padr√£o. Utilizado para detec√ß√£o de outliers (\|z\| \> 3) e normaliza√ß√£o de features. Exemplo em R: r Copy z \<- scale(df\$x)

Refer√™ncias gerais

Bergstra, J. & Bengio, Y. (2012). Random search for hyper-parameter optimization. JMLR, 13, 281-305.

Breiman, L. (2001). Random forests. Machine Learning, 45(1), 5-32.

Cover, T. & Hart, P. (1967). Nearest neighbor pattern classification. IEEE TIT, 13(1), 21-27.

Ioffe, S. & Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. ICML.
